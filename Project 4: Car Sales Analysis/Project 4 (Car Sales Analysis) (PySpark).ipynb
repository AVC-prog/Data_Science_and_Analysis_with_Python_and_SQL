{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11268112,"sourceType":"datasetVersion","datasetId":7043626}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Analysis (Using the PySpark Library)","metadata":{}},{"cell_type":"markdown","source":"- **The analysis of this dataset using the pandas library can be found here: https://github.com/AVC-prog/Data_Science_and_Analysis_with_Python_and_SQL/blob/main/Project%204%3A%20Car%20Sales%20Analysis/Project%204%20(Car%20Sales%20Analysis).ipynb**","metadata":{}},{"cell_type":"markdown","source":"# Prefactory Remarks","metadata":{}},{"cell_type":"markdown","source":"- **Create a virtual environment to download the packages**","metadata":{}},{"cell_type":"code","source":"# You don't have to do this, it's just safer.\n\n# Install virtualenv (virtual environment):\n\n# !pip install virtualenv\n\n# Create a virtual environment named \"myenv\":\n\n# !python -m venv myenv\n\n# Activate the virtual environment:\n\n# myenv\\Scripts\\activate (Windows)\n# source myenv/bin/activate (macOS/Linux)\n\n# Upgrade pip and install essential data science libraries inside the virtual environment:\n\n# !myenv/bin/python -m pip install --upgrade pip  \n# !myenv/bin/python -m pip install numpy pandas matplotlib seaborn scikit-learn scipy statsmodels jupyterlab plotly openpyxl xlrd tensorflow keras torch torchvision pyspark ipykernel\n\n# Add the virtual environment as a Jupyter kernel:\n\n# !myenv/bin/python -m ipykernel install --user --name=myenv --display-name \"Python (myenv)\"\n\n# Deactivate the virtual environment (Run this in the terminal):\n\n# deactivate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **Libraries we might need to install or upgrade**","metadata":{}},{"cell_type":"code","source":"# If you don't care to create a virtual environment, here is what you need to do to download the libraries\n\n# Run these directly in a cell to download the libraries:\n\n#!pip install tensorflow\n#!pip install pyspark\n#!pip install scikit-optimize (for skopt)\n#!pip install missingno\n#!pip install seaborn\n#!pip install numpy\n#!pip install pandas\n#!pip install matplotlib\n#!pip install scikit-learn\n\n# To update them, run this (with your desired library):\n\n#!pip install --upgrade scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **Tips for rearranging your Notebook**","metadata":{}},{"cell_type":"markdown","source":"- Hold ctrl+shift and click on the various cells you want to move, then press the arrow keys to move them up or down.","metadata":{}},{"cell_type":"markdown","source":"## 1. Visualize the Data","metadata":{}},{"cell_type":"markdown","source":"- [x] **Start the PySpark session and view the data.**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *  \nfrom pyspark.sql.types import *  \n\nspark = SparkSession.builder.appName(\"Test_CS\").getOrCreate()\n\n\ndf = spark.read.csv(\"/kaggle/input/car-sales/car_sales.csv\", header=True, inferSchema=True)\n\ndf.show(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:48:48.160168Z","iopub.execute_input":"2025-04-04T02:48:48.160539Z","iopub.status.idle":"2025-04-04T02:48:48.814614Z","shell.execute_reply.started":"2025-04-04T02:48:48.160506Z","shell.execute_reply":"2025-04-04T02:48:48.813205Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Clean the Data","metadata":{}},{"cell_type":"markdown","source":"- [x] **Check if there are any NaN values. Use imputation to fill them in as necessary.**","metadata":{}},{"cell_type":"code","source":"# Let's put them in a dictionary, just like we've done in the pandas notebook\n\nnot_numbers = {}\n\nfor column in df.columns:\n    not_numbers[column] = df.filter(df[column].isNull()).count()\n\nprint(not_numbers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:48:48.816094Z","iopub.execute_input":"2025-04-04T02:48:48.816732Z","iopub.status.idle":"2025-04-04T02:48:51.899167Z","shell.execute_reply.started":"2025-04-04T02:48:48.816666Z","shell.execute_reply":"2025-04-04T02:48:51.898115Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Check if there are any missing values. (empty slots)**","metadata":{}},{"cell_type":"code","source":"# Again, let's print the values out in a dictionary, just like in the pandas notebook\n\nempty_values = {}\n\nfor column in df.columns:\n    empty_values[column] = df.filter(df[column] == \"\").count()\n\nprint(empty_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:48:51.901000Z","iopub.execute_input":"2025-04-04T02:48:51.901390Z","iopub.status.idle":"2025-04-04T02:48:54.252400Z","shell.execute_reply.started":"2025-04-04T02:48:51.901352Z","shell.execute_reply":"2025-04-04T02:48:54.250950Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Disassociate the date column into day,month, and year.**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import functions as F\n\ndf = df.withColumn(\"Date\", F.to_date(df[\"Date\"], \"MM/dd/yyyy\"))\n\ndf = df.withColumn(\"week_day\", F.date_format(df[\"Date\"], \"EEEE\"))  \ndf = df.withColumn(\"day\", F.dayofmonth(df[\"Date\"]))  \ndf = df.withColumn(\"month\", F.date_format(df[\"Date\"], \"MMMM\"))  \ndf = df.withColumn(\"year\", F.year(df[\"Date\"]))  \n\ndf.show(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:59.752644Z","iopub.execute_input":"2025-04-04T02:49:59.753105Z","iopub.status.idle":"2025-04-04T02:50:00.367844Z","shell.execute_reply.started":"2025-04-04T02:49:59.753065Z","shell.execute_reply":"2025-04-04T02:50:00.365813Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Rename the columns to more SQL-friendly names.**","metadata":{}},{"cell_type":"code","source":"df = df.withColumnRenamed('Car_id', 'car_id') \\\n       .withColumnRenamed('Customer Name', 'customer_name') \\\n       .withColumnRenamed('Annual Income', 'annual_income') \\\n       .withColumnRenamed('Dealer_Name', 'dealer_name') \\\n       .withColumnRenamed('Company', 'company') \\\n       .withColumnRenamed('Model', 'model') \\\n       .withColumnRenamed('Engine', 'engine') \\\n       .withColumnRenamed('Transmission', 'transmission') \\\n       .withColumnRenamed('Color', 'color') \\\n       .withColumnRenamed('`Price ($)`', 'price') \\\n       .withColumnRenamed('Dealer_No', 'dealer_no') \\\n       .withColumnRenamed('Body Style', 'body_style') \\\n       .withColumnRenamed('Phone', 'phone') \\\n       .withColumnRenamed('Dealer_Region', 'dealer_region') \\\n       .withColumnRenamed('Date', 'date') \\\n       .withColumnRenamed('Gender', 'gender')\n\ndf.show(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:50:27.469477Z","iopub.execute_input":"2025-04-04T02:50:27.469835Z","iopub.status.idle":"2025-04-04T02:50:28.003755Z","shell.execute_reply.started":"2025-04-04T02:50:27.469808Z","shell.execute_reply":"2025-04-04T02:50:28.001743Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Save the clean csv file.**","metadata":{}},{"cell_type":"code","source":"# output_path = \"/kaggle/working/cleaned_car_sales.csv\"\n# df.write.option(\"header\", \"true\").csv(output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Exploratory Data Analysis (EDA) and Visualizations","metadata":{}},{"cell_type":"markdown","source":"- **The visualizations have been done in the pandas notebook, and given that the code would be the exact same here, I will exempt myself from repeating them.**","metadata":{}},{"cell_type":"markdown","source":"- [x] **Create a dictionary that shows the value counts for each element in each categorical column.**","metadata":{}},{"cell_type":"code","source":"categorical_columns = [\"Gender\", \"Company\", \"Model\", \"Engine\", \"Transmission\", \"Color\", \"Body Style\", \"Dealer_Region\"]\n\ncategory_counts = {}\n\nfor col in categorical_columns:\n    counts_df = df.groupBy(col).count()\n    category_counts[col] = {row[col]: row[\"count\"] for row in counts_df.collect()}\n\nprint(category_counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Most sales on weekends?**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import to_date, date_format, col, count\n\ndf = df.withColumn(\"date\", to_date(col(\"date\"), \"M/d/yyyy\"))\ndf = df.withColumn(\"week_day\", date_format(col(\"date\"), \"EEEE\"))\nsales_by_weekday = df.groupBy(\"week_day\").agg(count(\"*\").alias(\"total_sales\"))\n\nsales_by_weekday.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Create a new column called 'buy_prob' that is the probability of buying. It should follow the formula: $P = (\\textrm{income} - 3 * \\textrm{price}) / \\textrm{income}$**","metadata":{}},{"cell_type":"code","source":"# Income must me 3*car price for it to be considered \n# and whatever is beyond that reflects the probability to buy (if p is negative just put 0)\n\nfrom pyspark.sql.functions import col, when, round\n\n# Define the buy probability formula:\n# P = (income - 3 * price) / income\n# If P < 0, set it to 0\n\ndf = df.withColumn(\"buy_prob\",round(\n        when((col(\"annual_income\") - 3 * col(\"price\")) / col(\"annual_income\") < 0, 0)\n        .otherwise((col(\"annual_income\") - 3 * col(\"price\")) / col(\"annual_income\")),\n        2))\n\ndf.select(\"annual_income\", \"price\", \"buy_prob\").show(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Science (Using the PySpark Library)","metadata":{}},{"cell_type":"markdown","source":"## 4. Inferential Statistics","metadata":{}},{"cell_type":"markdown","source":"- **PySpark alone doesn’t offer all statistical tests natively, so we'll use some pandas as well.**","metadata":{}},{"cell_type":"markdown","source":"- [x] **Use a $\\chi^2$ test to compare gender and body style.**","metadata":{}},{"cell_type":"code","source":"from scipy.stats import chi2_contingency\n\ncontingency_table = pd.crosstab(pdf[\"gender\"], pdf[\"body_style\"])\nchi2, p, dof, expected = chi2_contingency(contingency_table)\n\nprint(\"Chi² Test (Gender vs Body Style):\")\nprint(f\"Chi² = {chi2:.2f}, p-value = {p:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Use the t-test to compare the genders**","metadata":{}},{"cell_type":"code","source":"from scipy.stats import ttest_ind\n\nmale_prices = pdf[pdf[\"gender\"] == \"Male\"][\"price\"]\nfemale_prices = pdf[pdf[\"gender\"] == \"Female\"][\"price\"]\n\nt_stat, p_value = ttest_ind(male_prices, female_prices, equal_var=False)\n\nprint(\"\\nT-Test (Price: Male vs Female):\")\nprint(f\"t-statistic = {t_stat:.2f}, p-value = {p_value:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Use the ANOVA to compare prices across dealer regions.**","metadata":{}},{"cell_type":"code","source":"from scipy.stats import f_oneway\n\nregion_groups = [group[\"price\"].values for name, group in pdf.groupby(\"dealer_region\")]\nanova_stat, anova_p = f_oneway(*region_groups)\n\nprint(\"\\nANOVA (Price by Dealer Region):\")\nprint(f\"F-statistic = {anova_stat:.2f}, p-value = {anova_p:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Principal Component Analysis (PCA)","metadata":{}},{"cell_type":"markdown","source":"- [x] **Reduce the dimensionality of the dataset while keeping the most important information.**","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA, StringIndexer, OneHotEncoder\nfrom pyspark.ml import Pipeline\nimport matplotlib.pyplot as plt\nimport pyspark.sql.functions as F\n\ncols_to_drop = ['price', 'car_id', 'date', 'customer_name', 'phone']\ndf_clean = df.drop(*cols_to_drop)\n\ncategorical_cols = [col for col, dtype in df_clean.dtypes if dtype == 'string']\nnumerical_cols = [col for col, dtype in df_clean.dtypes if dtype != 'string']\n\nindexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"skip\") for col in categorical_cols]\nencoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_ohe\") for col in categorical_cols]\n\nfeature_cols = [col + \"_ohe\" for col in categorical_cols] + numerical_cols\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n\npca = PCA(k=2, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n\npipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, pca])\nmodel = pipeline.fit(df_clean)\npca_result = model.transform(df_clean)\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DoubleType\n\nget_pc1 = udf(lambda v: float(v[0]), DoubleType())\nget_pc2 = udf(lambda v: float(v[1]), DoubleType())\n\npca_df = pca_result.withColumn(\"pc1\", get_pc1(\"pca_features\")) \\\n                   .withColumn(\"pc2\", get_pc2(\"pca_features\"))\n\npca_df = pca_df.withColumn(\"row_index\", F.monotonically_increasing_id())\nprice_df = df.select(\"price\").withColumn(\"row_index\", F.monotonically_increasing_id())\npca_with_price = pca_df.join(price_df, on=\"row_index\").select(\"pc1\", \"pc2\", \"price\")\n\npandas_data = pca_with_price.toPandas()\n\nplt.figure(figsize=(8, 6))\nplt.scatter(pandas_data['pc1'], pandas_data['pc2'], c=pandas_data['price'], cmap='viridis', alpha=0.5)\nplt.title(\"PCA: Car Sales Dataset (PySpark)\")\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.colorbar(label='Price ($)')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:08.455703Z","iopub.execute_input":"2025-04-04T02:49:08.456143Z","iopub.status.idle":"2025-04-04T02:49:13.118527Z","shell.execute_reply.started":"2025-04-04T02:49:08.456099Z","shell.execute_reply":"2025-04-04T02:49:13.114836Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. KMeans Clustering","metadata":{}},{"cell_type":"markdown","source":"- [x] **Use a KMeans clustering model to cluster cars based on price into \"low price\" and \"high price\".**","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.sql.functions import col, when\nimport matplotlib.pyplot as plt\n\nvec_assembler = VectorAssembler(inputCols=[\"price\"], outputCol=\"features\")\ndf_vector = vec_assembler.transform(df)\n\nkmeans = KMeans(k=2, seed=42, featuresCol=\"features\", predictionCol=\"cluster\")\nmodel = kmeans.fit(df_vector)\ndf_clustered = model.transform(df_vector)\n\ncluster_avg = df_clustered.groupBy(\"cluster\").avg(\"price\").orderBy(\"avg(price)\")\ncluster_avg.show()\n\nlow_price_cluster = cluster_avg.first()[\"cluster\"]\n\ndf_labeled = df_clustered.withColumn(\n    \"price_segment\",\n    when(col(\"cluster\") == low_price_cluster, \"Low Price\").otherwise(\"High Price\")\n)\n\npandas_data = df_labeled.select(\"price\", \"cluster\").toPandas()\n\nplt.figure(figsize=(10, 5))\nplt.scatter(pandas_data[\"price\"], [0] * len(pandas_data), c=pandas_data[\"cluster\"], cmap=\"Set2\", s=100)\nplt.title(\"KMeans Clustering of Car Prices (PySpark)\")\nplt.xlabel(\"Price ($)\")\nplt.yticks([])\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:41.376319Z","iopub.execute_input":"2025-04-04T02:49:41.376771Z","iopub.status.idle":"2025-04-04T02:49:41.458027Z","shell.execute_reply.started":"2025-04-04T02:49:41.376729Z","shell.execute_reply":"2025-04-04T02:49:41.456286Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Linear Regression","metadata":{}},{"cell_type":"markdown","source":"- [x] **Use a linear regression model to predict the car price using the Engine, Body Style, Color, Dealer Region, Car Model, Company**","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import RegressionEvaluator\nimport matplotlib.pyplot as plt\n\nselected_cols = [\"engine\", \"body_style\", \"color\", \"dealer_region\", \"model\", \"company\", \"price\"]\ndf_lr = df.select(*selected_cols)\n\ncategorical_cols = [\"engine\", \"body_style\", \"color\", \"dealer_region\", \"model\", \"company\"]\nindexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid='skip') for col in categorical_cols]\nencoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_ohe\") for col in categorical_cols]\n\nohe_cols = [col + \"_ohe\" for col in categorical_cols]\nassembler = VectorAssembler(inputCols=ohe_cols, outputCol=\"features\")\n\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"price\", predictionCol=\"prediction\")\n\npipeline = Pipeline(stages=indexers + encoders + [assembler, lr])\ntrain_data, test_data = df_lr.randomSplit([0.8, 0.2], seed=42)\nmodel = pipeline.fit(train_data)\npredictions = model.transform(test_data)\n\nevaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n\nmae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\nmse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\nrmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\nr2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\nprint(f\"R-squared: {r2}\")\n\npandas_data = predictions.select(\"price\", \"prediction\").toPandas()\n\nplt.figure(figsize=(10, 5))\nplt.scatter(pandas_data[\"price\"], pandas_data[\"prediction\"])\nplt.plot([pandas_data[\"price\"].min(), pandas_data[\"price\"].max()],\n         [pandas_data[\"price\"].min(), pandas_data[\"price\"].max()],\n         color='red', linestyle='--')\nplt.title(\"Actual vs Predicted Car Prices (PySpark)\")\nplt.xlabel(\"Actual Prices ($)\")\nplt.ylabel(\"Predicted Prices ($)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:49.200187Z","iopub.execute_input":"2025-04-04T02:49:49.200644Z","iopub.status.idle":"2025-04-04T02:49:49.410006Z","shell.execute_reply.started":"2025-04-04T02:49:49.200607Z","shell.execute_reply":"2025-04-04T02:49:49.408305Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"- [] **Use a Logistic Regression model to predict the car model based on Transmission, and Body Style.**","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nindexer = StringIndexer(inputCol=\"Transmission\", outputCol=\"label\")\n\nindexer_model = StringIndexer(inputCol=\"Model\", outputCol=\"Model_index\")\nindexer_body_style = StringIndexer(inputCol=\"Body Style\", outputCol=\"Body_Style_index\")\nindexer_dealer_region = StringIndexer(inputCol=\"Dealer_Region\", outputCol=\"Dealer_Region_index\")\n\nencoder_model = OneHotEncoder(inputCol=\"Model_index\", outputCol=\"Model_vec\")\nencoder_body_style = OneHotEncoder(inputCol=\"Body_Style_index\", outputCol=\"Body_Style_vec\")\nencoder_dealer_region = OneHotEncoder(inputCol=\"Dealer_Region_index\", outputCol=\"Dealer_Region_vec\")\n\nassembler = VectorAssembler(inputCols=[\"Model_vec\", \"Body_Style_vec\", \"Dealer_Region_vec\"], outputCol=\"features\")\n\nlog_reg = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n\npipeline = Pipeline(stages=[indexer, indexer_model, indexer_body_style, indexer_dealer_region, \n                            encoder_model, encoder_body_style, encoder_dealer_region, assembler, log_reg])\n\ntrain_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\nmodel = pipeline.fit(train_data)\n\npredictions = model.transform(test_data)\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(f\"Test Accuracy: {accuracy}\")\n\ny_true = predictions.select(\"label\").rdd.flatMap(lambda x: x).collect()\ny_pred = predictions.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n\nfrom sklearn.metrics import confusion_matrix, classification_report\ncm = confusion_matrix(y_true, y_pred)\ncr = classification_report(y_true, y_pred)\n\nprint(\"Classification Report:\\n\", cr)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix - Logistic Regression\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:03:26.893043Z","iopub.execute_input":"2025-04-04T03:03:26.893491Z","iopub.status.idle":"2025-04-04T03:03:28.476691Z","shell.execute_reply.started":"2025-04-04T03:03:26.893425Z","shell.execute_reply":"2025-04-04T03:03:28.474769Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Gradient Boost","metadata":{}},{"cell_type":"markdown","source":"- [x] **Use a gradient boosting model to predict a customer's gender based on their Annual Income, Car Model, Body Style, and Dealer Region.**","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import col\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfeatures = [\"annual_income\", \"model\", \"body_style\", \"dealer_region\"]\ntarget = \"gender\"\n\nindexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\", handleInvalid=\"keep\") for col in features if col != \"annual_income\"]\nencoders = [OneHotEncoder(inputCol=col+\"_idx\", outputCol=col+\"_ohe\") for col in features if col != \"annual_income\"]\n\nohe_cols = [col+\"_ohe\" for col in features if col != \"annual_income\"]\ninput_features = ohe_cols + [\"annual_income\"]\nassembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")\n\npipeline = Pipeline(stages=indexers + encoders + [assembler])\ndf_prepped = pipeline.fit(df).transform(df)\n\npandas_df = df_prepped.select(\"features\", target).toPandas()\n\nX = np.array(pandas_df[\"features\"].tolist())\ny = pandas_df[target].values\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nreport = classification_report(y_test, y_pred, target_names=le.classes_)\nmatrix = confusion_matrix(y_test, y_pred)\n\nprint(\"Classification Report:\\n\", report)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\nplt.xlabel(\"Predicted Gender\")\nplt.ylabel(\"Actual Gender\")\nplt.title(\"Confusion Matrix - XGBoost Gender Prediction\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:13.124702Z","iopub.status.idle":"2025-04-04T02:49:13.125142Z","shell.execute_reply":"2025-04-04T02:49:13.124956Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Neural Networks","metadata":{}},{"cell_type":"markdown","source":"- [x] **Use a classification neural networks model to predict the \"will buy\" column (that you'll create)**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import when\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml import Pipeline\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\n\ndf = df.withColumn(\"will_buy\", when(df[\"price\"] > 30000, 1).otherwise(0))\n\nfeatures = [\"engine\", \"body_style\", \"color\", \"company\", \"annual_income\"]\ntarget = \"will_buy\"\n\nindexers = [StringIndexer(inputCol=col, outputCol=col + \"_idx\", handleInvalid=\"keep\") for col in features if col != \"annual_income\"]\nencoders = [OneHotEncoder(inputCol=col + \"_idx\", outputCol=col + \"_ohe\") for col in features if col != \"annual_income\"]\n\nohe_cols = [col + \"_ohe\" for col in features if col != \"annual_income\"]\ninput_cols = ohe_cols + [\"annual_income\"]\nassembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n\npipeline = Pipeline(stages=indexers + encoders + [assembler])\ndf_prepped = pipeline.fit(df).transform(df)\n\npandas_df = df_prepped.select(\"features\", \"will_buy\").toPandas()\n\nX = np.array(pandas_df[\"features\"].tolist())\ny = np.array(pandas_df[\"will_buy\"].tolist())\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n\ny_pred = (model.predict(X_test) > 0.5).astype(int)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Val Accuracy')\nplt.title(\"Accuracy over Epochs\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.title(\"Loss over Epochs\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:13.126322Z","iopub.status.idle":"2025-04-04T02:49:13.126795Z","shell.execute_reply":"2025-04-04T02:49:13.126602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SQL Queries (Using the PySpark Library)","metadata":{}},{"cell_type":"markdown","source":"- **This csv file has over 23000 rows, therefore I decided to do the SQL queries here as opposed to uploading a file to MySQL.**","metadata":{}},{"cell_type":"markdown","source":"- [x] **Start by fixing the columns to SQL-friendly names (again).**","metadata":{}},{"cell_type":"code","source":"# df = spark.read.csv(\"/kaggle/input/car-sales/car_sales.csv\", header=True, inferSchema=True)\n\ndf = df.withColumnRenamed('Car_id', 'car_id') \\\n       .withColumnRenamed('Customer Name', 'customer_name') \\\n       .withColumnRenamed('Annual Income', 'annual_income') \\\n       .withColumnRenamed('Dealer_Name', 'dealer_name') \\\n       .withColumnRenamed('Company', 'company') \\\n       .withColumnRenamed('Model', 'model') \\\n       .withColumnRenamed('Engine', 'engine') \\\n       .withColumnRenamed('Transmission', 'transmission') \\\n       .withColumnRenamed('Color', 'color') \\\n       .withColumnRenamed('`Price ($)`', 'price') \\\n       .withColumnRenamed('Dealer_No', 'dealer_no') \\\n       .withColumnRenamed('Body Style', 'body_style') \\\n       .withColumnRenamed('Phone', 'phone') \\\n       .withColumnRenamed('Dealer_Region', 'dealer_region') \\\n       .withColumnRenamed('Date', 'date') \\\n       .withColumnRenamed('Gender', 'gender')\n\n\n\nprint(df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:13.128333Z","iopub.status.idle":"2025-04-04T02:49:13.128810Z","shell.execute_reply":"2025-04-04T02:49:13.128626Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Create a SQL query that retrieves the total number of cars sold.**","metadata":{}},{"cell_type":"code","source":"df.createOrReplaceTempView(\"car_sales\")\n\ntotal_cars_sold = spark.sql(\"select count(*) as total_cars_sold from car_sales\")\ntotal_cars_sold.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:13.129509Z","iopub.status.idle":"2025-04-04T02:49:13.129951Z","shell.execute_reply":"2025-04-04T02:49:13.129770Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Create a SQL query that retrieves the total sales by dealer.**","metadata":{}},{"cell_type":"code","source":"total_sales_by_dealer = spark.sql(\"\"\"\n    select dealer_name, sum(price) as total_sales\n    from car_sales\n    group by dealer_name\n    order by total_sales desc\n\"\"\")\ntotal_sales_by_dealer.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:13.130803Z","iopub.status.idle":"2025-04-04T02:49:13.131209Z","shell.execute_reply":"2025-04-04T02:49:13.131035Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Top 10 companies that sold the most cars.**","metadata":{}},{"cell_type":"code","source":"spark.sql(\"\"\"\n    select company, count(*) as cars_sold\n    from car_sales\n    group by company\n    order by cars_sold desc\n    limit 10\n\"\"\").show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:13.131953Z","iopub.status.idle":"2025-04-04T02:49:13.132417Z","shell.execute_reply":"2025-04-04T02:49:13.132175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Top 10 most bought models.**","metadata":{}},{"cell_type":"code","source":"spark.sql(\"\"\"\n    select model, count(*) as bought\n    from car_sales\n    group by model\n    order by count desc\n    limit 10\n\"\"\").show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:13.134012Z","iopub.status.idle":"2025-04-04T02:49:13.134450Z","shell.execute_reply":"2025-04-04T02:49:13.134258Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Pick the 2nd and 3rd most bought car colors.**","metadata":{}},{"cell_type":"code","source":"# spark.sql(\"\"\"\n#     select color, count(*) as color_count\n#     from car_sales\n#     group by color\n#     order by color_count desc\n#     limit 1,2\n# \"\"\").show()                     \n\n# This limit function (skipping the 1st and showing the subsequent two) works in MySQL but not with PySpark\n\nspark.sql(\"\"\"\n    select color, color_count\n    from (select color, count(*) as color_count,\n        row_number() over (order by count(*) desc) as rank\n        from car_sales\n        group by color) \n    where rank in (2,3)\n\"\"\").show()                      # I used a subquery here instead of a cte (common table expression)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:13.135059Z","iopub.status.idle":"2025-04-04T02:49:13.135752Z","shell.execute_reply":"2025-04-04T02:49:13.135299Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Most sold cars by company and model.**","metadata":{}},{"cell_type":"code","source":"spark.sql(\"\"\"\n    select company, model, count(*) as total_sold\n    from car_sales\n    group by company, model\n    order by total_sold desc, company desc\n    limit 10\n\"\"\").show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:13.137501Z","iopub.status.idle":"2025-04-04T02:49:13.137940Z","shell.execute_reply":"2025-04-04T02:49:13.137757Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Find the top 5 dealers with the highest total sales, along with their most sold car model and the number of times it was sold.**","metadata":{}},{"cell_type":"code","source":"spark.sql(\"\"\"\n    with dealer_sales as (\n        select dealer_name, model, count(*) as model_count,\n        sum(price) over (partition by dealer_name) as total_sales,\n        row_number() over (partition by dealer_name order by count(*) desc) as rank\n        from car_sales\n        group by dealer_name, model)\n    select dealer_name, total_sales, model as most_sold_model, model_count\n    from dealer_sales\n    where rank in (1,2,3,4,5)\n    order by total_sales desc\n\"\"\").show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:13.138973Z","iopub.status.idle":"2025-04-04T02:49:13.139640Z","shell.execute_reply":"2025-04-04T02:49:13.139421Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Find the average price of cars for each dealer region, but only for dealers who have sold at least 50 cars.**","metadata":{}},{"cell_type":"code","source":"spark.sql(\"\"\"\n    with region_sales as (\n        select dealer_region, count(*) as total_cars_sold, avg(price) as avg_price\n        from car_sales\n        group by dealer_region)\n    select dealer_region, avg_price\n    from region_sales\n    where total_cars_sold >= 50\n    order by avg_price desc\n\"\"\").show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:13.140648Z","iopub.status.idle":"2025-04-04T02:49:13.141350Z","shell.execute_reply":"2025-04-04T02:49:13.141138Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Find the top 3 customers who have spent the most money, and show the models they purchased along with total amount spent.**","metadata":{}},{"cell_type":"code","source":"spark.sql(\"\"\"\n    with customer_spending as (\n        select customer_name, sum(price) as total_spent,\n        collect_list(model) as models_purchased\n        from car_sales\n        group by customer_name)\n    select customer_name, total_spent, array_join(models_purchased, ', ') as models\n    from customer_spending\n    order by total_spent desc\n    limit 3\n\"\"\").show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:49:13.142915Z","iopub.status.idle":"2025-04-04T02:49:13.143415Z","shell.execute_reply":"2025-04-04T02:49:13.143153Z"}},"outputs":[],"execution_count":null}]}