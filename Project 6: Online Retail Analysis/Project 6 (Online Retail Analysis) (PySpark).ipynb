{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11267917,"sourceType":"datasetVersion","datasetId":7043487}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Analysis (Using the Pyspark Library)","metadata":{}},{"cell_type":"markdown","source":"- **The notebook with this analysis done in pandas can be found here: https://github.com/AVC-prog/Data_Science_and_Analysis_with_Python_and_SQL/blob/main/Project%206%3A%20Online%20Retail%20Analysis/Project%206%20-%20Online%20Retail%20Analysis.ipynb**","metadata":{}},{"cell_type":"markdown","source":"# Prefactory Remarks","metadata":{}},{"cell_type":"code","source":"# You don't have to do this, it's just safer.\n\n# Install virtualenv (virtual environment):\n\n# !pip install virtualenv\n\n# Create a virtual environment named \"myenv\":\n\n# !python -m venv myenv\n\n# Activate the virtual environment:\n\n# myenv\\Scripts\\activate (Windows)\n# source myenv/bin/activate (macOS/Linux)\n\n# Upgrade pip and install essential data science libraries inside the virtual environment:\n\n# !myenv/bin/python -m pip install --upgrade pip  \n# !myenv/bin/python -m pip install numpy pandas matplotlib seaborn scikit-learn scipy statsmodels jupyterlab plotly openpyxl xlrd tensorflow keras torch torchvision pyspark ipykernel\n\n# Add the virtual environment as a Jupyter kernel:\n\n# !myenv/bin/python -m ipykernel install --user --name=myenv --display-name \"Python (myenv)\"\n\n# Deactivate the virtual environment (Run this in the terminal):\n\n# deactivate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Necessary packages to use**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, avg, when\nfrom pyspark.sql.types import IntegerType, StringType, BooleanType\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, PCA\nfrom pyspark.ml.classification import (DecisionTreeClassifier,RandomForestClassifier,\n    MultilayerPerceptronClassifier,GBTClassifier)\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.tuning import TrainValidationSplit, CrossValidator, ParamGridBuilder\nfrom pyspark.sql import functions as F\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Visualize the data","metadata":{}},{"cell_type":"markdown","source":"- [x] **Start the PySpark session and view the data.**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *  \nfrom pyspark.sql.types import *  \n\nspark = SparkSession.builder.appName(\"Test_OR\").getOrCreate()\n\n\ndf = spark.read.csv(\"/kaggle/input/online-retail/online_retail.csv\", header=True, inferSchema=True)\n\ndf.show(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:19:35.688336Z","iopub.execute_input":"2025-04-04T03:19:35.689089Z","iopub.status.idle":"2025-04-04T03:19:37.726654Z","shell.execute_reply.started":"2025-04-04T03:19:35.688962Z","shell.execute_reply":"2025-04-04T03:19:37.725502Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Clean the Data","metadata":{}},{"cell_type":"markdown","source":"- [x] **Check for NaN values. Use imputation to fill in the slots for those values.**","metadata":{}},{"cell_type":"code","source":"# Let's put them in a dictionary, just like we've done in the pandas notebook\n\nnot_numbers = {}\n\nfor column in df.columns:\n    not_numbers[column] = df.filter(df[column].isNull()).count()\n\nprint(not_numbers)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Check for missing values (empty slots). Use imputation to fill in the slots for those values.**","metadata":{}},{"cell_type":"code","source":"# Again, let's print the values out in a dictionary, just like in the pandas notebook\n\nempty_values = {}\n\nfor column in df.columns:\n    empty_values[column] = df.filter(df[column] == \"\").count()\n\nprint(empty_values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Check for erronious values if there are any.**","metadata":{}},{"cell_type":"code","source":"# typically negative quantity represents returns and negative prices represent devolutions, but we'll ignore it here.\n\ndf.groupBy(\"Country\").count().show()\ndf.groupBy(\"Price\").count().show()\ndf.groupBy(\"Quantity\").count().show()\n\ndf = df.withColumn(\"Quantity\", abs(col(\"Quantity\")))   \n\ndf.groupBy(\"Quantity\").count().show()\n\ndf = df.withColumn(\"Price\", abs(col(\"Price\")))\n\ndf.groupBy(\"Price\").count().show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:22:09.167658Z","iopub.execute_input":"2025-04-04T03:22:09.168140Z","iopub.status.idle":"2025-04-04T03:22:16.908062Z","shell.execute_reply.started":"2025-04-04T03:22:09.168042Z","shell.execute_reply":"2025-04-04T03:22:16.906964Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Fix the invoice date column by separating it into date and time.**","metadata":{}},{"cell_type":"code","source":"df = df.withColumn('Invoice_Date', to_timestamp(col('InvoiceDate'), 'MM/dd/yyyy HH:mm'))\n\ndf = df.withColumn('Date', date_format(col('Invoice_Date'), 'yyyy-MM-dd')) \ndf = df.withColumn('Time', date_format(col('Invoice_Date'), 'HH:mm:ss'))  \n\ndf = df.withColumn('Date', to_date(col('Date'), 'yyyy-MM-dd'))\n\ndf.printSchema()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:28:01.858162Z","iopub.execute_input":"2025-04-04T03:28:01.858488Z","iopub.status.idle":"2025-04-04T03:28:01.938242Z","shell.execute_reply.started":"2025-04-04T03:28:01.858462Z","shell.execute_reply":"2025-04-04T03:28:01.937048Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Save the clean file.**","metadata":{}},{"cell_type":"code","source":"output_path = \"/kaggle/working/cleaned_onine_retail.csv\"\ndf.write.option(\"header\", \"true\").csv(output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. String Manipulation","metadata":{}},{"cell_type":"markdown","source":"- [x] **Choose stockcodes that only have 5 digits and the letter \"A\" after.**","metadata":{}},{"cell_type":"code","source":"stock_df_vA = df\n\nstock_df_vA = stock_df_vA.filter(col(\"StockCode\").rlike(\"^\\d{5}A$\"))\n\nstock_df_vA.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [] **Now, choose 5 digits followed by the letter \"B\" in the stockcode.**","metadata":{}},{"cell_type":"code","source":"stock_df_vB = df\n\nstock_df_vB = stock_df_vB.filter(col(\"StockCode\").rlike(\"^\\d{5}B$\"))\n\nstock_df_vB.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Exploratory Data Analysis (EDA) and Visualizations","metadata":{}},{"cell_type":"markdown","source":"- **The visualizations have already been performed in the pandas version of this dataset, so I'll refrain from repeating them.**","metadata":{}},{"cell_type":"markdown","source":"- [] **Sales trends over time.**","metadata":{}},{"cell_type":"code","source":"import matplotlib.animation as animation\n\ndf = df.withColumn(\"InvoiceTimestamp\", to_timestamp(\"InvoiceDate\", \"MM/dd/yyyy H:mm\"))\n\ndf = df.withColumn(\"Year\", year(\"InvoiceTimestamp\")) \\\n       .withColumn(\"Month\", month(\"InvoiceTimestamp\")) \\\n       .withColumn(\"Weekday\", dayofweek(\"InvoiceTimestamp\")) \\\n       .withColumn(\"Hour\", hour(\"InvoiceTimestamp\"))\n\ndf = df.withColumn(\"Revenue\", col(\"Quantity\") * col(\"Price\"))\n\nmonthly_sales = df.groupBy(\"Year\", \"Month\").agg(_sum(\"Revenue\").alias(\"TotalRevenue\")).orderBy(\"Year\", \"Month\")\nmonthly_sales.show()\n\nmonthly_sales_pd = monthly_sales.toPandas()\nmonthly_sales_pd[\"Date\"] = pd.to_datetime(dict(year=monthly_sales_pd.Year, month=monthly_sales_pd.Month, day=1))\n\n\nmonthly_sales_pd = monthly_sales_pd.sort_values(\"Date\").reset_index(drop=True)\n\nfig, ax = plt.subplots(figsize=(12,6))\nline, = ax.plot([], [], lw=2)\nax.set_xlim(monthly_sales_pd[\"Date\"].min(), monthly_sales_pd[\"Date\"].max())\nax.set_ylim(0, monthly_sales_pd[\"TotalRevenue\"].max() * 1.1)\nax.set_title(\"Animated Monthly Sales Revenue\")\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Revenue\")\n\ndef init():\n    line.set_data([], [])\n    return line,\n\ndef animate(i):\n    x = monthly_sales_pd[\"Date\"][:i+1]\n    y = monthly_sales_pd[\"TotalRevenue\"][:i+1]\n    line.set_data(x, y)\n    return line,\n\nani = animation.FuncAnimation(fig, animate, init_func=init, frames=len(monthly_sales_pd), interval=300, blit=True)\n\nfrom IPython.display import HTML\nHTML(ani.to_jshtml())\n\n# To save:\n# ani.save(\"monthly_sales_animation.mp4\", writer='ffmpeg')\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [] **Perform a country level analysis**","metadata":{}},{"cell_type":"code","source":"country_sales = df.groupBy(\"Country\").agg(\n    _sum(\"Revenue\").alias(\"TotalRevenue\"),\n    countDistinct(\"Customer ID\").alias(\"NumCustomers\"),\n    countDistinct(\"Invoice\").alias(\"NumOrders\")\n).orderBy(col(\"TotalRevenue\").desc())\n\ncountry_sales.show(10)\ncountry_sales_pd = country_sales.toPandas()\n\ntop_countries = country_sales_pd.head(10)\nplt.figure(figsize=(10,6))\nplt.barh(top_countries['Country'], top_countries['TotalRevenue'])\nplt.title(\"Top Countries by Revenue\")\nplt.gca().invert_yaxis()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Perform a basket analysis, as in discover product bundling opportunities for Common product pairs or groups and potential cross-sell/upsell opportunities.**","metadata":{}},{"cell_type":"code","source":"basket_df = df.select(\"Invoice\", \"StockCode\").dropna().dropDuplicates()\n\nbasket_items = basket_df.groupBy(\"Invoice\").agg(collect_set(\"StockCode\").alias(\"Items\"))\nbasket_items.show(5, truncate=False)\n\nbasket_items_pd = basket_items.toPandas()\n\nfrom mlxtend.preprocessing import TransactionEncoder\nte = TransactionEncoder()\nte_ary = te.fit(basket_items_pd[\"Items\"]).transform(basket_items_pd[\"Items\"])\nbasket_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n\nfrom mlxtend.frequent_patterns import apriori, association_rules\nfrequent_itemsets = apriori(basket_encoded, min_support=0.01, use_colnames=True)\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n\nrules.sort_values(by=\"lift\", ascending=False).head(10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Science (Using the PySpark Library)","metadata":{}},{"cell_type":"markdown","source":"## 5. Inferential Statistics","metadata":{}},{"cell_type":"markdown","source":"- [] **Store the outliers of the data in the version A and B stock dataframes by using the typical interquartile range.**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nspark = SparkSession.builder.appName(\"OutliersDetection\").getOrCreate()\n\n\nQ1_A, Q3_A = stock_df_vA.approxQuantile(\"Price\", [0.25, 0.75], 0.01)\nIQR_A = Q3_A - Q1_A\n\nout_A_df = stock_df_vA.filter((col(\"Price\") < (Q1_A - 1.5 * IQR_A)) | (col(\"Price\") > (Q3_A + 1.5 * IQR_A)))\n\nQ1_B, Q3_B = stock_df_vB.approxQuantile(\"Price\", [0.25, 0.75], 0.01)\nIQR_B = Q3_B - Q1_B\n\nout_B_df = stock_df_vB.filter((col(\"Price\") < (Q1_B - 1.5 * IQR_B)) | (col(\"Price\") > (Q3_B + 1.5 * IQR_B)))\n\nout_A_df.describe().show()\nout_B_df.describe().show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [] **Use a $\\chi^2$ test to check if Country and Revenue Class (high/low revenue) are related.**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum as _sum\nimport pandas as pd\nfrom scipy.stats import chi2_contingency\n\ndf = df.withColumn(\"Revenue\", col(\"Quantity\") * col(\"Price\"))\n\ninvoice_revenue_df = df.groupBy(\"Invoice\", \"Country\").agg(\n    _sum(\"Revenue\").alias(\"TotalInvoiceRevenue\")\n)\n\ninvoice_pd = invoice_revenue_df.toPandas()\nmedian_revenue = invoice_pd[\"TotalInvoiceRevenue\"].median()\n\ninvoice_pd[\"RevenueClass\"] = invoice_pd[\"TotalInvoiceRevenue\"].apply(\n    lambda x: \"High\" if x >= median_revenue else \"Low\"\n)\n\ncontingency_table = pd.crosstab(invoice_pd[\"Country\"], invoice_pd[\"RevenueClass\"])\nprint(\"Contingency Table (Top 5 rows):\")\nprint(contingency_table.head())\n\nchi2, p, dof, expected = chi2_contingency(contingency_table)\n\nprint(f\"\\nChi-squared statistic: {chi2:.2f}\")\nprint(f\"Degrees of freedom: {dof}\")\nprint(f\"P-value: {p:.4f}\")\n\nalpha = 0.05\nif p < alpha:\n    print(\"Statistically significant relationship between Country and Revenue Class.\")\nelse:\n    print(\"No statistically significant relationship between Country and Revenue Class.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [] **Use the Pearson/Spearman test to check if the quantity and price columns are related.**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nimport pandas as pd\nfrom scipy.stats import pearsonr, spearmanr\n\n\nfiltered_df = df.filter(\n    (col(\"Quantity\").isNotNull()) & \n    (col(\"Price\").isNotNull()) &\n    (col(\"Quantity\") > 0) &\n    (col(\"Price\") > 0)\n)\n\nfiltered_pd = filtered_df.select(\"Quantity\", \"Price\").toPandas()\n\npearson_corr, pearson_p = pearsonr(filtered_pd[\"Quantity\"], filtered_pd[\"Price\"])\n\nspearman_corr, spearman_p = spearmanr(filtered_pd[\"Quantity\"], filtered_pd[\"Price\"])\n\nprint(\"Pearson Correlation:\")\nprint(f\"  Correlation Coefficient: {pearson_corr:.4f}\")\nprint(f\"  P-value: {pearson_p:.4f}\")\nprint(\"  â†’ Linear relationship\" if pearson_p < 0.05 else \"  â†’ Not statistically significant\")\n\nprint(\"Spearman Correlation:\")\nprint(f\"  Correlation Coefficient: {spearman_corr:.4f}\")\nprint(f\"  P-value: {spearman_p:.4f}\")\nprint(\"  â†’ Monotonic relationship\" if spearman_p < 0.05 else \"  â†’ Not statistically significant\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [] **Use a t-test to compare the average price of purchases between two specific countries.**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n\n\ncountry_a = \"United Kingdom\"\ncountry_b = \"Germany\"\n\nfiltered_df = df.filter(\n    (col(\"Country\").isin([country_a, country_b])) &\n    (col(\"Price\").isNotNull()) &\n    (col(\"Price\") > 0)\n)\n\nprice_pd = filtered_df.select(\"Country\", \"Price\").toPandas()\n\ngroup_a = price_pd[price_pd[\"Country\"] == country_a][\"Price\"]\ngroup_b = price_pd[price_pd[\"Country\"] == country_b][\"Price\"]\n\nt_stat, p_value = ttest_ind(group_a, group_b, equal_var=False)\n\nprint(f\"ðŸŽ¯ Comparing average prices between '{country_a}' and '{country_b}'\\n\")\nprint(f\"T-statistic: {t_stat:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\nalpha = 0.05\nif p_value < alpha:\n    print(\"Result: Statistically significant difference in average prices between the two countries.\")\nelse:\n    print(\"Result: No statistically significant difference in average prices between the two countries.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [] **Use an ANOVA test to check whether average quantity differs across multiple countries.**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nimport pandas as pd\nfrom scipy.stats import f_oneway\n\nspark = SparkSession.builder.appName(\"CountryQuantityANOVA\").getOrCreate()\n\ndf = spark.read.csv(\"your_file.csv\", header=True, inferSchema=True)\n\nfiltered_df = df.filter(\n    (col(\"Country\").isNotNull()) &\n    (col(\"Quantity\").isNotNull()) &\n    (col(\"Quantity\") > 0)\n)\n\ntop_countries = (filtered_df.groupBy(\"Country\").count()\n    .orderBy(\"count\", ascending=False)\n    .limit(5).toPandas()[\"Country\"].tolist())\n\nfiltered_df = filtered_df.filter(col(\"Country\").isin(top_countries))\n\nquantity_pd = filtered_df.select(\"Country\", \"Quantity\").toPandas()\n\ngroups = [quantity_pd[quantity_pd[\"Country\"] == country][\"Quantity\"] for country in top_countries]\n\nf_stat, p_value = f_oneway(*groups)\n\nprint(\"ANOVA: Does mean quantity differ by country?\\n\")\nprint(f\"F-statistic: {f_stat:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\nalpha = 0.05\nif p_value < alpha:\n    print(\"There is a statistically significant difference in quantity between at least two countries.\")\nelse:\n    print(\"No statistically significant difference in quantity across countries.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Linear Regression ","metadata":{}},{"cell_type":"markdown","source":"- [] ****","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.sql.functions import col\nimport numpy as np\n\ndf = spark.read.csv(\"path_to_file.csv\", header=True, inferSchema=True)\n\ndf = df.select(\"Quantity\", \"Price\")\n\nassembler = VectorAssembler(inputCols=[\"Quantity\"], outputCol=\"features\")\ndf = assembler.transform(df)\n\ntrain_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"Price\")\n\nlr_model = lr.fit(train_data)\n\npredictions = lr_model.transform(test_data)\n\nevaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Price\", metricName=\"r2\")\nr2 = evaluator.evaluate(predictions)\n\nevaluator.setMetricName(\"mae\")\nmae = evaluator.evaluate(predictions)\n\nevaluator.setMetricName(\"mse\")\nmse = evaluator.evaluate(predictions)\n\nrmse = np.sqrt(mse)\n\nprint(\"R2 Score:\", r2)\nprint(\"MAE:\", mae)\nprint(\"MSE:\", mse)\nprint(\"RMSE:\", rmse)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"- [] ****","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import col\n\nspark = SparkSession.builder.appName(\"OnlineRetailData\").getOrCreate()\n\ndf = spark.read.csv(\"path_to_file.csv\", header=True, inferSchema=True)\n\ndf = df.withColumn(\"Revenue Class\", (col(\"Price\") * col(\"Quantity\") > 100).cast(\"int\"))  # Example rule for classification\n\nindexer = StringIndexer(inputCol=\"Country\", outputCol=\"Country_encoded\")\ndf = indexer.fit(df).transform(df)\n\nassembler = VectorAssembler(inputCols=[\"Country_encoded\", \"Price\", \"Quantity\"], outputCol=\"features\")\ndf = assembler.transform(df)\n\ntrain_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"Revenue Class\")\nlr_model = lr.fit(train_data)\n\npredictions = lr_model.transform(test_data)\n\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"Revenue Class\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\npredictions.select(\"Revenue Class\", \"prediction\").show(10)\n\nprint(f\"Accuracy: {accuracy}\")\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"Revenue Class\", metricName=\"f1\")\nf1 = evaluator.evaluate(predictions)\nprint(f\"F1 Score: {f1}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. KMeans Clustering","metadata":{}},{"cell_type":"markdown","source":"- [] ****","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml import Pipeline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nspark = SparkSession.builder.appName(\"KMeansClustering\").getOrCreate()\n\ncombined_outliers = out_A_df.union(out_B_df)\n\ndf = combined_outliers.select(\"Price\", \"Quantity\")\n\nassembler = VectorAssembler(inputCols=[\"Price\", \"Quantity\"], outputCol=\"features\")\ndf = assembler.transform(df)\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\ndf = scaler.fit(df).transform(df)\n\nkmeans = KMeans(k=6, featuresCol=\"scaled_features\", predictionCol=\"k_means\")\nmodel = kmeans.fit(df)\npredictions = model.transform(df)\n\npredictions_pd = predictions.select(\"Price\", \"Quantity\", \"k_means\").toPandas()\n\ninertia = []\nfor k in range(1, 11):\n    kmeans = KMeans(k=k, featuresCol=\"scaled_features\", predictionCol=\"k_means\")\n    model = kmeans.fit(df)\n    predictions = model.transform(df)\n    inertia.append(predictions.select(\"k_means\").count())  \n\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, 11), inertia, marker='o')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method to Find Optimal k')\nplt.show()\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=predictions_pd, x='Price', y='Quantity', hue='k_means', palette='viridis')\nplt.xlabel('Price')\nplt.ylabel('Quantity')\nplt.title('K-Means Clustering of Outliers')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [] **Use a silhouette score to see if the clusters are, in fact, well separated enough.**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.evaluation import ClusteringEvaluator\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nevaluator = ClusteringEvaluator(predictionCol=\"k_means\", featuresCol=\"scaled_features\", metricName=\"silhouette\")\nsilhouette_avg = evaluator.evaluate(predictions)\n\nprint(f\"Silhouette Score (k=6): {silhouette_avg}\")\n\npredictions_pd = predictions.select(\"Price\", \"Quantity\", \"k_means\").toPandas()\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=predictions_pd, x='Price', y='Quantity', hue='k_means', palette='viridis')\nplt.xlabel('Price')\nplt.ylabel('Quantity')\nplt.title('K-Means Clustering of Outliers')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SQL Queries (Using the PySpark Library)","metadata":{}},{"cell_type":"markdown","source":"- It's more useful to do the SQL queries here using PySpark, since MySQL takes a long time to import over half a million rows of data.","metadata":{}},{"cell_type":"markdown","source":"- [x] **Create a SQL query that retrives the top 10 Products by Total Revenue**","metadata":{}},{"cell_type":"code","source":"\ndf.createOrReplaceTempView(\"retail_data\")\n\ntop_products_query = \"\"\"\nSELECT\n    Description,\n    StockCode,\n    SUM(Quantity * Price) AS TotalRevenue,\n    SUM(Quantity) AS TotalUnitsSold\nFROM retail_data\nWHERE Quantity > 0 AND Price > 0\nGROUP BY Description, StockCode\nORDER BY TotalRevenue DESC\nLIMIT 10\n\"\"\"\n\ntop_products_df = spark.sql(top_products_query)\ntop_products_df.show(truncate=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [] **Create a SQL query that retrieves all of the version A products. (StockCode has 5 digits and the letter \"A\").**","metadata":{}},{"cell_type":"code","source":"\n\ndf.createOrReplaceTempView(\"retail_data\")\n\nversion_a_query = \"\"\"\nSELECT *\nFROM retail_data\nWHERE StockCode RLIKE '^[0-9]{5}A$'\n\"\"\"\n\nversion_a_df = spark.sql(version_a_query)\n\nversion_a_df.show(10) \n\nprint(f\"Version A product count: {version_a_df.count()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [] **Create a SQL query that retrieves all of the version B products. (StockCode has 5 digits and the letter \"B\").**","metadata":{}},{"cell_type":"code","source":"df.createOrReplaceTempView(\"retail_data\")\n\nversion_b_query = \"\"\"\nSELECT *\nFROM retail_data\nWHERE StockCode RLIKE '^[0-9]{5}B$'\n\"\"\"\n\nversion_b_df = spark.sql(version_b_query)\n\nversion_b_df.show(10) \n\nprint(f\"Version B product count: {version_b_df.count()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- [x] **Don't forget to stop the spark session.**","metadata":{}},{"cell_type":"code","source":"# spark.stop()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}